# Unique code of this Data Store Server. Not more than 40 characters.
data-store-server-code = standard

# The root directory of the data store
storeroot-dir = targets/store

# The directory where the command queue file is located; defaults to storeroot-dir 
commandqueue-dir =

# Port
port = 8889

# Session timeout in minutes
session-timeout = 30

# Set to 'false' for development/testing without deployed server. In this mode datastore will not use
# SSL when connecting to openbis. Otherwise all 'keystore' properties need to be set for SSL connection 
# (default when use-ssl property is not set so there is no need to specify it on production servers).
use-ssl = false

# Path, password and key password for SSL connection
#keystore.path = dist/etc/openBIS.keystore
#keystore.password = changeit
#keystore.key-password = changeit

# The check interval (in seconds)
check-interval = 5

# The time-out for clean up work in the shutdown sequence (in seconds).
# Note that that the maximal time for the shutdown sequence to complete can be as large 
# as twice this time.
shutdown-timeout = 2

# If free disk space goes below value defined here, a notification email will be sent.
# Value must be specified in kilobytes (1048576 = 1024 * 1024 = 1GB). If no high water mark is
# specified or if value is negative, the system will not be watching.
highwater-mark = 1048576

# If a data set is successfully registered it sends out an email to the registrator. 
# If this property is not specified, no email is sent to the registrator. This property
# does not affect the mails which are sent, when the data set could not be registered.
notify-successful-registration = false

# The URL of the openBIS server
server-url = http://localhost:8888

# The username to use when contacting the openBIS server
username = etlserver

# The password to use when contacting the openBIS server
password = doesnotmatter

#
# CIFEX configuration: Only needed if data export should work without the user having to type in 
# his password. Note that in in order for this to work the CIFEX server needs to be configured to 
# allow calling setSessionUser() from the IP address of this data store server, see configuration 
# option allowed-ips-for-set-session-user in CIFEX's service.properties    
#

# The admin username to use when contacting the CIFEX server 
cifex-admin-username =

# The admin password to use when contacting the CIFEX server 
cifex-admin-password =

# The base URL for Web client access.
download-url = http://localhost:8889

# SMTP properties (must start with 'mail' to be considered).
mail.smtp.host = file://targets/email
mail.from = datastore_server@localhost
mail.smtp.user = 
mail.smtp.password = 
# If this property is set a test e-mail will be sent to the specified user after DSS successfully started-up.
mail.test.address = test@localhost

# Maximum number of retries if renaming failed.
# renaming.failure.max-retries = 12

# The number of milliseconds to wait before retrying to execute the renaming process.
# renaming.failure.millis-to-sleep = 5000

# Globally used separator character which separates entities in a data set file name 
data-set-file-name-entity-separator = _

# The period of no write access that needs to pass before an incoming data item is considered 
# complete and ready to be processed (in seconds) [default: 300]. 
# Valid only when auto-detection method is used to determine if an incoming data are ready to be processed.
quiet-period = 3

# ---------------------------------------------------------------------------
# reporting and processing plugins configuration
# ---------------------------------------------------------------------------

# Comma separated names of reporting plugins. Each plugin should have configuration properties prefixed with its name.
# If name has 'default-' prefix it will be used by default in data set Data View.
reporting-plugins = demo-reporter, tsv-viewer, csv-viewer

# Label of the plugin which will be shown for the users.
demo-reporter.label = Show Dataset Size
# Comma separated list of dataset type codes which can be handled by this plugin.
demo-reporter.dataset-types = HCS_IMAGE
# Plugin class specification (together with the list of packages this class belongs to).
demo-reporter.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoReportingPlugin
# The property file. Its content will be passed as a parameter to the plugin.
demo-reporter.properties-file = 

tsv-viewer.label = TSV View 
tsv-viewer.dataset-types = TSV
tsv-viewer.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
tsv-viewer.properties-file =

csv-viewer.label = CSV View 
csv-viewer.dataset-types = CSV
csv-viewer.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
csv-viewer.separator = ,

# Comma separated names of processing plugins. Each plugin should have configuration properties prefixed with its name.
processing-plugins = demo-processor

# The configuration of the processing plugin is the same as the reporting plugins configuration. 
demo-processor.label = Demo Processing
demo-processor.dataset-types = HCS_IMAGE
demo-processor.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoProcessingPlugin
demo-processor.properties-file = 

# Data set validators used to accept or reject data sets to be registered.
# Comma separated list of validator definitions.
data-set-validators = validator

# Definition of data set validator 'validator'
validator.data-set-type = HCS_IMAGE
validator.path-patterns = **/*.txt
validator.columns = id, description, size
validator.id.header-pattern = ID
validator.id.mandatory = true
validator.id.order = 1
validator.id.value-type = unique
validator.description.header-pattern = Description
validator.description.value-type = string
validator.description.value-pattern = .{0,100}
validator.size.header-pattern = A[0-9]+
validator.size.can-define-multiple-columns = true
validator.size.allow-empty-values = true
validator.size.value-type = numeric
validator.site.value-range = [0,Infinity)

# ---------------------------------------------------------------------------

# Comma separated names of processing threads. Each thread should have configuration properties prefixed with its name.
# E.g. 'code-extractor' property for the thread 'my-etl' should be specified as 'my-etl.code-extractor'
inputs=main-thread, tsv-thread, csv-thread

# ---------------------------------------------------------------------------
# Dataset uploader 'main-thread' configuration
# ---------------------------------------------------------------------------

# The directory to watch for incoming data.
main-thread.incoming-dir = targets/incoming

# Specifies what should happen if an error occurs during dataset processing. 
# By default this flag is set to false and user has to modify the 'faulty paths file' 
# each time the faulty dataset should be processed again.
# Set this flag to true if the processing should be repeated after some time without manual intervention.
# Note that this can increase the server load.
# main-thread.reprocess-faulty-datasets = false

# Determines when the incoming data should be considered complete and ready to be processed.
# Allowed values: 
#  - auto-detection - when no write access will be detected for a specified 'quite-period'
#  - marker-file		- when an appropriate marker file for the data exists. 
# The default value is 'marker-file'.
main-thread.incoming-data-completeness-condition = marker-file

# The store format that should be applied in the incoming directory.
main-thread.incoming-dir.format = 

# The extractor plugin class to use for code extraction
main-thread.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
# Separator used to extract the barcode in the data set file name
main-thread.data-set-info-extractor.entity-separator = ${data-set-file-name-entity-separator}
#main-thread.data-set-info-extractor.space-code = CISD
# Location of file containing data set properties 
#main-thread.data-set-info-extractor.data-set-properties-file-name = data-set.properties

main-thread.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
main-thread.type-extractor.file-format-type = TIFF
main-thread.type-extractor.locator-type = RELATIVE_LOCATION
main-thread.type-extractor.data-set-type = HCS_IMAGE

# The storage processor (IStorageProcessor implementation)
#main-thread.storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor
main-thread.storage-processor = ch.systemsx.cisd.etlserver.BDSStorageProcessor
main-thread.storage-processor.version = 1.1
main-thread.storage-processor.sampleTypeCode = CELL_PLATE
main-thread.storage-processor.sampleTypeDescription = Screening Plate
main-thread.storage-processor.format = HCS_IMAGE V1.0
main-thread.storage-processor.number_of_channels = 2
main-thread.storage-processor.contains_original_data = TRUE
main-thread.storage-processor.well_geometry = 3x3
main-thread.storage-processor.file-extractor = ch.systemsx.cisd.etlserver.imsb.HCSImageFileExtractor

# The directory to watch for incoming data.
tsv-thread.incoming-dir = targets/incoming-tsv
tsv-thread.incoming-data-completeness-condition = auto-detection
tsv-thread.strip-file-extension = true
tsv-thread.storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor
# The extractor plugin class to use for code extraction
tsv-thread.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
# Separator used to extract the barcode in the data set file name
tsv-thread.data-set-info-extractor.entity-separator = ${data-set-file-name-entity-separator}
tsv-thread.data-set-info-extractor.strip-file-extension = true
#main-thread.data-set-info-extractor.space-code = CISD
# Location of file containing data set properties 
#main-thread.data-set-info-extractor.data-set-properties-file-name = data-set.properties

tsv-thread.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
tsv-thread.type-extractor.file-format-type = TSV
tsv-thread.type-extractor.locator-type = RELATIVE_LOCATION
tsv-thread.type-extractor.data-set-type = TSV

csv-thread.incoming-dir = targets/incoming-csv
csv-thread.incoming-data-completeness-condition = auto-detection
csv-thread.storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor
# The extractor plugin class to use for code extraction
csv-thread.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
# Separator used to extract the barcode in the data set file name
csv-thread.data-set-info-extractor.entity-separator = ${data-set-file-name-entity-separator}
csv-thread.data-set-info-extractor.strip-file-extension = true
csv-thread.data-set-info-extractor.space-code = CISD
# Location of file containing data set properties 
#main-thread.data-set-info-extractor.data-set-properties-file-name = data-set.properties
csv-thread.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
csv-thread.type-extractor.file-format-type = CSV
csv-thread.type-extractor.locator-type = RELATIVE_LOCATION
csv-thread.type-extractor.data-set-type = CSV

# ---------------------------------------------------------------------------
# (optional) archiver configuration
# ---------------------------------------------------------------------------

# Configuration of an archiver task. All properties are prefixed with 'archiver.'.

# Archiver class specification (together with the list of packages this class belongs to).
archiver.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoArchiver

# ---------------------------------------------------------------------------
# maintenance plugins configuration
# ---------------------------------------------------------------------------

# Comma separated names of maintenance plugins.  
# Each plugin should have configuration properties prefixed with its name.
# Mandatory properties for each <plugin> include: 
#   <plugin>.class - Fully qualified plugin class name
#   <plugin>.interval - The time between plugin executions (in seconds)
# Optional properties for each <plugin> include:
#   <plugin>.start - Time of the first execution (HH:mm)
#   <plugin>.execute-only-once - If true the task will be executed exactly once, 
#                                interval will be ignored. By default set to false.
maintenance-plugins = auto-archiver

# Performs automatic archivization of 'ACTIVE' data sets based on their properties
auto-archiver.class = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask
# The time between subsequent archivizations (in seconds)
auto-archiver.interval = 10
#  Time of the first execution (HH:mm)
auto-archiver.start = 23:00
# following properties are optional
# only data sets of specified type will be archived  
#auto-archiver.data-set-type = UNKNOWN
# only data sets that are older than specified number of days will be archived (default = 0)  
#auto-archiver.older-than = 90
# fully qualified class name of a policy that additionally filters data sets to be filtered
#auto-archiver.policy.class = ch.systemsx.cisd.etlserver.plugins.DummyAutoArchiverPolicy