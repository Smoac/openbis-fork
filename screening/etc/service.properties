# Unique code of this Data Store Server. Not more than 40 characters.
data-store-server-code = dss-screening

# host name of the machine on which the datastore server is running
host-address = http://localhost

# parent directory of the store directory and all the dropboxes
root-dir = targets

# The root directory of the data store
storeroot-dir = ${root-dir}/store

core-plugins-folder=../screening/source/core-plugins

# The directory where the command queue file is located; defaults to storeroot-dir 
commandqueue-dir =

# Port
port = 8889
use-ssl = false

# Session timeout in minutes
session-timeout = 30

# Path to the keystore
keystore.path = dist/etc/openBIS.keystore
# Password of the keystore
keystore.password = changeit
# Key password of the keystore
keystore.key-password = changeit

# The check interval (in seconds)
check-interval = 5

# The time-out for clean up work in the shutdown sequence (in seconds).
# Note that that the maximal time for the shutdown sequence to complete can be as large 
# as twice this time.
# Remark: On a network file system, it is not recommended to turn this value to something 
# lower than 180.
shutdown-timeout = 2

# If free disk space goes below value defined here, a notification email will be sent.
# Value must be specified in kilobytes (1048576 = 1024 * 1024 = 1GB). If no high water mark is
# specified or if value is negative, the system will not be watching.
highwater-mark = 1000

# If a data set is successfully registered it sends out an email to the registrator. 
# If this property is not specified, no email is sent to the registrator. This property
# does not affect the mails which are sent, when the data set could not be registered.
notify-successful-registration = false

# The URL of the openBIS server
server-url = ${host-address}:8888

# The username to use when contacting the openBIS server
username = etlserver

# The password for the etlserver user who contacts the openBIS server
password = etlserver_password

# The base URL for Web client access to the data store server.
download-url = ${host-address}:8889

# SMTP properties (must start with 'mail' to be considered).
# The current configuration saves the emails in the file system in the root directory 
mail.smtp.host = file://${root-dir}/emails
# mail.smtp.host = localhost
# mail.from = datastore_server@ethz.ch
# mail.smtp.user = 
# mail.smtp.password = 

dss-registration-log-dir = ${root-dir}/dss-registration-logs
dss-recovery-state-dir = ${root-dir}/recovery-state

# ---------------------------------------------------------------------------
# (optional) archiver configuration
# ---------------------------------------------------------------------------

# Configuration of an archiver task. All properties are prefixed with 'archiver.'.

# Archiver class specification (together with the list of packages this class belongs to).
#archiver.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoArchiver

# ---------------------------------------------------------------------------
#                      INTERNAL CONFIGURATION, 
# Do not change this part unless you are developing openBIS extensions.
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# screening database specification
# ---------------------------------------------------------------------------

data-sources = path-info-db

# Data source for pathinfo database
path-info-db.version-holder-class = ch.systemsx.cisd.openbis.dss.generic.shared.PathInfoDatabaseVersionHolder
path-info-db.databaseEngineCode = postgresql
path-info-db.basicDatabaseName = pathinfo
path-info-db.databaseKind = dev
path-info-db.scriptFolder = ../datastore_server/source/sql

imaging-database.kind = dev
screening-sql-root-folder = source/

# ---------------------------------------------------------------------------
# reporting and processing plugins configuration
# ---------------------------------------------------------------------------

# Comma separated names of reporting plugins. Each plugin should have configuration properties prefixed with its name.
reporting-plugins =  plate-image-analysis-graph, csv-viewer

plate-image-analysis-graph.label = Image Analysis Graphs
plate-image-analysis-graph.dataset-types = HCS_IMAGE_ANALYSIS_DATA, HCS_ANALYSIS_PER_GENE
plate-image-analysis-graph.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.ImageAnalysisGraphReportingPlugin
plate-image-analysis-graph.servlet-path = datastore_server_graph/
plate-image-analysis-graph.properties-file = etc/tabular-data-graph.properties
plate-image-analysis-graph.servlet.class = ch.systemsx.cisd.openbis.dss.generic.server.TabularDataGraphServlet
plate-image-analysis-graph.servlet.path = /${plate-image-analysis-graph.servlet-path}*
plate-image-analysis-graph.servlet.properties-file = ${plate-image-analysis-graph.properties-file}


csv-viewer.label = CSV View 
csv-viewer.dataset-types = HCS_IMAGE_ANALYSIS_DATA
csv-viewer.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
csv-viewer.separator = ,


# ---------------------------------------------------------------------------

maintenance-plugins = path-info-deletion, post-registration

# Maintenance task (performed only once) to create paths of existing data sets in pathinfo database
path-info-feeding.class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask
path-info-feeding.execute-only-once = true

# Maintenance task for deleting entries in pathinfo database after deletion of data sets
path-info-deletion.class = ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask
path-info-deletion.interval = 120
path-info-deletion.data-source = path-info-db
path-info-deletion.data-set-table-name = data_sets
path-info-deletion.data-set-perm-id = CODE

# Maintenance task for post registration of all paths of a freshly registered data set to be fed into pathinfo database  
post-registration.class = ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask
post-registration.interval = 30
post-registration.cleanup-tasks-folder = targets/cleanup-tasks
# The following date should the day when the DDS is started up the first time with PathInfoDatabaseFeedingTask.
# After PathInfoDatabaseFeedingTask has been performed it can be removed and the following line can be deleted.
#post-registration.ignore-data-sets-before-date = 2011-04-18
post-registration.last-seen-data-set-file = targets/last-seen-data-set
post-registration.post-registration-tasks = pathinfo-feeding
post-registration.pathinfo-feeding.class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask

# ---------------------------------------------------------------------------
#                      DROPBOXES CONFIGURATION 
# ---------------------------------------------------------------------------

incoming-root-dir = ${root-dir}

# True if incoming directories should be created on server startup if they don't exist. 
# Default - false (server will fail at startup if one of incoming directories doesn't exist). 
incoming-dir-create = true

# Globally used separator character which separates entities in a data set file name 
data-set-file-name-entity-separator = _

# The period of no write access that needs to pass before an incoming data item is considered 
# complete and ready to be processed (in seconds) [default: 300]. 
# Valid only when auto-detection method is used to determine if an incoming data are ready to be processed.
quiet-period = 3

# code of the default space in openBIS to which the data will be imported
import-space-code = TEST

# Comma separated names of processing threads. Each thread should have configuration properties prefixed with its name.
# E.g. 'code-extractor' property for the thread 'my-etl' should be specified as 'my-etl.code-extractor'
inputs=merged-channels-images, image-analysis-results, genedata-merged-channels-images, genedata-image-analysis-results, timepoint-images

# ---------------------------------------------------------------------------

# The directory to watch for incoming data.
merged-channels-images.incoming-dir = ${incoming-root-dir}/incoming-images-merged-channels
merged-channels-images.incoming-data-completeness-condition = auto-detection

# The extractor class to use for code extraction
merged-channels-images.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
merged-channels-images.data-set-info-extractor.entity-separator = .
merged-channels-images.data-set-info-extractor.index-of-sample-code = 0
merged-channels-images.data-set-info-extractor.index-of-data-producer-code = 
merged-channels-images.data-set-info-extractor.space-code = ${import-space-code}

# The extractor class to use for type extraction
merged-channels-images.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
merged-channels-images.type-extractor.file-format-type = JPG
merged-channels-images.type-extractor.locator-type = RELATIVE_LOCATION
merged-channels-images.type-extractor.data-set-type = HCS_IMAGE
merged-channels-images.type-extractor.is-measured = true

merged-channels-images.storage-processor = ch.systemsx.cisd.openbis.dss.etl.PlateStorageProcessor
# Should the thumbnails be generated? 
# It slows down the dataset registration, but increases the performance when the user wants to see the image. 
# Can be 'true' or 'false', 'false' is the default value
merged-channels-images.storage-processor.generate-thumbnails = false
# Thumbnails size in pixels
# merged-channels-images.storage-processor.thumbnail-max-width = 300
# merged-channels-images.storage-processor.thumbnail-max-height = 200
# DEPRECATED: use 'channel-codes' and 'channel-labels' instead
#merged-channels-images.storage-processor.channel-names = gfp, dapi
# Codes of the channels in which images have been acquired. Allowed characters: [A-Z0-9_]. Number and order of entries must be consistent with 'channel-labels'.
merged-channels-images.storage-processor.channel-codes = GFP, DAPI
# Labels of the channels in which images have been acquired. Number and order of entries must be consistent with 'channel-codes'.
merged-channels-images.storage-processor.channel-labels = Gfp, Dapi
# Format: [width]>x[height], e.g. 3x4. Specifies the grid into which a microscope divided the well to acquire images.
merged-channels-images.storage-processor.well_geometry = 3x3
# implementation of the IHCSImageFileExtractor interface which maps images to the location on the plate and particular channel
# Here: the extractor requireds that each image name should adhere to the schema:
#     <any-text>_<plate-code>_<well-code>_<tile-code>_<channel-name>.<allowed-image-extension>
merged-channels-images.storage-processor.file-extractor = ch.systemsx.cisd.openbis.dss.etl.HCSImageFileExtractor
# specification of the imaging db
merged-channels-images.storage-processor.data-source = imaging-db
# Optional comma separated list of color components. 
# Available values: RED, GREEN or BLUE. 
# If specified then the channels are extracted from the color components and override 'file-extractor' results.
merged-channels-images.storage-processor.extract-single-image-channels = GREEN, BLUE


# ---------------------------------------------------------------------------

# The directory to watch for incoming data.
image-analysis-results.incoming-dir = ${incoming-root-dir}/incoming-analysis
image-analysis-results.incoming-data-completeness-condition = auto-detection

# The extractor class to use for code extraction
image-analysis-results.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
# Separator used to extract the barcode in the data set file name
image-analysis-results.data-set-info-extractor.entity-separator = .
image-analysis-results.data-set-info-extractor.index-of-sample-code = 0
image-analysis-results.data-set-info-extractor.space-code = ${import-space-code}

# The extractor class to use for type extraction
image-analysis-results.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
image-analysis-results.type-extractor.file-format-type = CSV
image-analysis-results.type-extractor.locator-type = RELATIVE_LOCATION
image-analysis-results.type-extractor.data-set-type = HCS_IMAGE_ANALYSIS_DATA
image-analysis-results.type-extractor.is-measured = false

# The storage processor (IStorageProcessor implementation)
image-analysis-results.storage-processor = ch.systemsx.cisd.openbis.dss.etl.featurevector.FeatureVectorStorageProcessor
image-analysis-results.storage-processor.processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor
image-analysis-results.storage-processor.data-source = imaging-db
# semi-colon (;) by default 
image-analysis-results.storage-processor.separator = ,
image-analysis-results.storage-processor.ignore-comments = true
image-analysis-results.storage-processor.well-name-row = row
image-analysis-results.storage-processor.well-name-col = col
image-analysis-results.storage-processor.well-name-col-is-alphanum = true

# ---------------------------------------------------------------------------
# GENEDATA formats
# ---------------------------------------------------------------------------

# The directory to watch for incoming data.
genedata-merged-channels-images.incoming-dir = ${incoming-root-dir}/incoming-images-genedata
genedata-merged-channels-images.incoming-data-completeness-condition = auto-detection

# The extractor class to use for code extraction
genedata-merged-channels-images.data-set-info-extractor = ch.systemsx.cisd.openbis.dss.etl.genedata.DataSetInfoExtractorForDataAcquisition
# Separator used to extract the barcode in the data set file name
genedata-merged-channels-images.data-set-info-extractor.entity-separator = .
genedata-merged-channels-images.data-set-info-extractor.index-of-sample-code = 0
genedata-merged-channels-images.data-set-info-extractor.index-of-data-producer-code = 
genedata-merged-channels-images.data-set-info-extractor.space-code = ${import-space-code}

# The extractor class to use for type extraction
genedata-merged-channels-images.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
genedata-merged-channels-images.type-extractor.file-format-type = JPG
genedata-merged-channels-images.type-extractor.locator-type = RELATIVE_LOCATION
genedata-merged-channels-images.type-extractor.data-set-type = HCS_IMAGE
genedata-merged-channels-images.type-extractor.is-measured = true

genedata-merged-channels-images.storage-processor = ch.systemsx.cisd.openbis.dss.etl.PlateStorageProcessor
genedata-merged-channels-images.storage-processor.well_geometry = 1x1
genedata-merged-channels-images.storage-processor.channel-codes = DAPI, GFP
genedata-merged-channels-images.storage-processor.channel-labels = Dapi, Gfp
# Available values: RED, BLUE or GREEN
genedata-merged-channels-images.storage-processor.extract-single-image-channels = BLUE, GREEN
genedata-merged-channels-images.storage-processor.deprecated-file-extractor = ch.systemsx.cisd.openbis.dss.etl.genedata.HCSImageFileExtractor
genedata-merged-channels-images.storage-processor.data-source = imaging-db

#  --------------------------------------------------------------

# The directory to watch for incoming data.
genedata-image-analysis-results.incoming-dir = ${incoming-root-dir}/incoming-analysis-genedata
genedata-image-analysis-results.incoming-data-completeness-condition = auto-detection

# The extractor class to use for code extraction
genedata-image-analysis-results.data-set-info-extractor = ch.systemsx.cisd.openbis.dss.etl.genedata.DataSetInfoExtractorForImageAnalysis
# Separator used to extract the barcode in the data set file name
genedata-image-analysis-results.data-set-info-extractor.entity-separator = .
genedata-image-analysis-results.data-set-info-extractor.index-of-sample-code = 0
genedata-image-analysis-results.data-set-info-extractor.space-code = ${import-space-code}

# The extractor class to use for type extraction
genedata-image-analysis-results.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
genedata-image-analysis-results.type-extractor.file-format-type = CSV
genedata-image-analysis-results.type-extractor.locator-type = RELATIVE_LOCATION
genedata-image-analysis-results.type-extractor.data-set-type = HCS_IMAGE_ANALYSIS_DATA
genedata-image-analysis-results.type-extractor.is-measured = false

# The storage processor (IStorageProcessor implementation)
genedata-image-analysis-results.storage-processor = ch.systemsx.cisd.openbis.dss.etl.genedata.FeatureStorageProcessor
genedata-image-analysis-results.storage-processor.processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor
genedata-image-analysis-results.storage-processor.data-source = imaging-db

# ---------------------------------------------------------------------------
# Timepoint Images
# ---------------------------------------------------------------------------

# The directory to watch for incoming data.
timepoint-images.incoming-dir = ${incoming-root-dir}/incoming-images-timepoints
timepoint-images.incoming-data-completeness-condition = auto-detection

# The extractor class to use for code extraction
timepoint-images.data-set-info-extractor = ch.systemsx.cisd.etlserver.DefaultDataSetInfoExtractor
timepoint-images.data-set-info-extractor.entity-separator = ${data-set-file-name-entity-separator}
timepoint-images.data-set-info-extractor.index-of-sample-code = 0
timepoint-images.data-set-info-extractor.space-code = ${import-space-code}

# The extractor class to use for type extraction
timepoint-images.type-extractor = ch.systemsx.cisd.etlserver.SimpleTypeExtractor
timepoint-images.type-extractor.file-format-type = TIFF
timepoint-images.type-extractor.locator-type = RELATIVE_LOCATION
timepoint-images.type-extractor.data-set-type = HCS_IMAGE
timepoint-images.type-extractor.is-measured = true

timepoint-images.storage-processor = ch.systemsx.cisd.openbis.dss.etl.PlateStorageProcessor
timepoint-images.storage-processor.generate-thumbnails = false
timepoint-images.storage-processor.channel-names = dia, epi
timepoint-images.storage-processor.well_geometry = 1x1
timepoint-images.storage-processor.file-extractor = ch.systemsx.cisd.openbis.dss.etl.dynamix.HCSImageFileExtractor
timepoint-images.storage-processor.data-source = imaging-db

